{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Balanced Configuration Analysis with Per-Bin Accuracy\n",
    "\n",
    "This notebook analyzes the hybrid balanced configuration that combines anti-overfitting features for magnitude with high-performance features for frequency.\n",
    "\n",
    "## Overview\n",
    "- Loads hybrid balanced trained models\n",
    "- Analyzes anti-overfitting effectiveness for magnitude\n",
    "- Evaluates high-performance features for frequency\n",
    "- Provides per-bin accuracy analysis\n",
    "- Compares Simple LSTM vs Attention LSTM performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hybrid Balanced Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hybrid balanced configuration\n",
    "with open('hybrid_balanced_config.json', 'r') as f:\n",
    "    hybrid_config = json.load(f)\n",
    "\n",
    "print(\"üî¨ HYBRID BALANCED CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Name: {hybrid_config['name']}\")\n",
    "print(f\"Description: {hybrid_config['description']}\")\n",
    "print(f\"Version: {hybrid_config['version']}\")\n",
    "print()\n",
    "\n",
    "print(\"üèóÔ∏è  MODEL ARCHITECTURE:\")\n",
    "for key, value in hybrid_config['model_architecture'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"‚öôÔ∏è  TRAINING PARAMETERS:\")\n",
    "for key, value in hybrid_config['training_parameters'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"ÔøΩÔøΩ LOSS WEIGHTS:\")\n",
    "for key, value in hybrid_config['loss_weights'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"üìä FREQUENCY SCALING:\")\n",
    "for key, value in hybrid_config['frequency_scaling'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Hybrid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ HYBRID FEATURES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"üéØ Magnitude (Anti-Overfitting Features):\")\n",
    "for feature, enabled in hybrid_config['hybrid_features']['magnitude_anti_overfitting'].items():\n",
    "    status = \"‚úÖ\" if enabled else \"‚ùå\"\n",
    "    print(f\"  {status} {feature}\")\n",
    "\n",
    "print(\"\\nüöÄ Frequency (High-Performance Features):\")\n",
    "for feature, enabled in hybrid_config['hybrid_features']['frequency_high_performance'].items():\n",
    "    status = \"‚úÖ\" if enabled else \"‚ùå\"\n",
    "    print(f\"  {status} {feature}\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Balanced Training Features:\")\n",
    "for feature, enabled in hybrid_config['hybrid_features']['balanced_training'].items():\n",
    "    status = \"‚úÖ\" if enabled else \"‚ùå\"\n",
    "    print(f\"  {status} {feature}\")\n",
    "\n",
    "print(f\"\\nüìä Expected Performance:\")\n",
    "print(f\"  Magnitude Accuracy: {hybrid_config['expected_behavior']['magnitude_accuracy_target']}\")\n",
    "print(f\"  Frequency Accuracy: {hybrid_config['expected_behavior']['frequency_accuracy_target']}\")\n",
    "print(f\"  Prediction Range: {hybrid_config['expected_behavior']['prediction_range_target']}\")\n",
    "print(f\"  Overfitting Indicators: {hybrid_config['expected_behavior']['overfitting_indicators']}\")\n",
    "print(f\"  Performance Balance: {hybrid_config['expected_behavior']['performance_balance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Output Directory (Use Hybrid Balanced Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use hybrid balanced results\n",
    "output_dir = \"data/results_hybrid_balanced_comparison\"\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Full path: {Path(output_dir).absolute()}\")\n",
    "print(f\"Results dir: {Path(output_dir) / 'results' / 'model_comparison'}\")\n",
    "print(f\"Results dir exists: {(Path(output_dir) / 'results' / 'model_comparison').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comparison_results(output_dir: str) -> Dict:\n",
    "    \"\"\"Load the comparison results from the output directory.\"\"\"\n",
    "    \n",
    "    results_dir = Path(output_dir) / \"results\" / \"model_comparison\"\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        print(f\"‚ùå Results directory not found: {results_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Found results directory: {results_dir}\")\n",
    "    \n",
    "    # List available files\n",
    "    available_files = list(results_dir.glob(\"*\"))\n",
    "    print(f\"ÔøΩÔøΩ Available files:\")\n",
    "    for file in available_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    # Load comparison metrics\n",
    "    metrics_file = results_dir / \"comparison_metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        print(f\"\\nüìä Loaded comparison metrics:\")\n",
    "        print(f\"  Simple LSTM - Magnitude Accuracy: {metrics['simple_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "        print(f\"  Simple LSTM - Frequency Accuracy: {metrics['simple_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "        print(f\"  Attention LSTM - Magnitude Accuracy: {metrics['attention_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "        print(f\"  Attention LSTM - Frequency Accuracy: {metrics['attention_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "    \n",
    "    # Check if trained models exist\n",
    "    simple_model_path = results_dir / \"simple_lstm_model.pth\"\n",
    "    attention_model_path = results_dir / \"attention_lstm_model.pth\"\n",
    "    \n",
    "    if simple_model_path.exists() and attention_model_path.exists():\n",
    "        print(f\"\\n‚úÖ Found trained models:\")\n",
    "        print(f\"  Simple LSTM: {simple_model_path}\")\n",
    "        print(f\"  Attention LSTM: {attention_model_path}\")\n",
    "        return {\n",
    "            'models_dir': str(results_dir),\n",
    "            'simple_model_path': str(simple_model_path),\n",
    "            'attention_model_path': str(attention_model_path),\n",
    "            'metrics': metrics if 'metrics' in locals() else None\n",
    "        }\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Trained models not found\")\n",
    "        return None\n",
    "\n",
    "# Load results if available\n",
    "if output_dir:\n",
    "    results = load_comparison_results(output_dir)\n",
    "else:\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Config Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    print(\"üî¨ HYBRID CONFIG PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Extract metrics\n",
    "    simple_mag_acc = metrics['simple_lstm_metrics']['magnitude_accuracy']\n",
    "    simple_freq_acc = metrics['simple_lstm_metrics']['frequency_accuracy']\n",
    "    attention_mag_acc = metrics['attention_lstm_metrics']['magnitude_accuracy']\n",
    "    attention_freq_acc = metrics['attention_lstm_metrics']['frequency_accuracy']\n",
    "    \n",
    "    print(\"üìä ACCURACY ANALYSIS:\")\n",
    "    print(f\"  Simple LSTM:     {simple_mag_acc:.3f} magnitude, {simple_freq_acc:.3f} frequency\")\n",
    "    print(f\"  Attention LSTM:  {attention_mag_acc:.3f} magnitude, {attention_freq_acc:.3f} frequency\")\n",
    "    \n",
    "    # Check anti-overfitting effectiveness\n",
    "    print(\"\\nüõ°Ô∏è ANTI-OVERFITTING CHECK:\")\n",
    "    max_freq_acc = max(simple_freq_acc, attention_freq_acc)\n",
    "    if max_freq_acc < 0.95:\n",
    "        print(f\"  ‚úÖ Frequency NOT overfitted: {max_freq_acc:.3f} < 0.95\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Frequency may be overfitted: {max_freq_acc:.3f} >= 0.95\")\n",
    "    \n",
    "    # Check performance balance\n",
    "    print(\"\\n‚öñÔ∏è PERFORMANCE BALANCE:\")\n",
    "    simple_total = simple_mag_acc + simple_freq_acc\n",
    "    attention_total = attention_mag_acc + attention_freq_acc\n",
    "    \n",
    "    if abs(simple_total - attention_total) < 0.1:\n",
    "        print(\"  ‚úÖ Models are well balanced\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Models show performance imbalance\")\n",
    "    \n",
    "    # Hybrid config success assessment\n",
    "    print(\"\\nüéØ HYBRID CONFIG SUCCESS:\")\n",
    "    success_criteria = [\n",
    "        max_freq_acc < 0.95,  # No overfitting\n",
    "        min(simple_mag_acc, attention_mag_acc) > 0.6,  # Good magnitude performance\n",
    "        min(simple_freq_acc, attention_freq_acc) > 0.7,  # Good frequency performance\n",
    "    ]\n",
    "    \n",
    "    success_count = sum(success_criteria)\n",
    "    if success_count == 3:\n",
    "        print(\"  üèÜ HYBRID CONFIG: FULL SUCCESS!\")\n",
    "    elif success_count == 2:\n",
    "        print(\"  üéØ HYBRID CONFIG: MOSTLY SUCCESSFUL\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è HYBRID CONFIG: NEEDS ADJUSTMENT\")\n",
    "    \n",
    "    # Detailed analysis\n",
    "    print(\"\\nüìã DETAILED ANALYSIS:\")\n",
    "    print(f\"  Magnitude Performance: {'‚úÖ Good' if min(simple_mag_acc, attention_mag_acc) > 0.6 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "    print(f\"  Frequency Performance: {'‚úÖ Good' if min(simple_freq_acc, attention_freq_acc) > 0.7 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "    print(f\"  Overfitting Prevention: {'‚úÖ Effective' if max_freq_acc < 0.95 else '‚ö†Ô∏è May be overfitting'}\")\n",
    "    print(f\"  Model Balance: {'‚úÖ Balanced' if abs(simple_total - attention_total) < 0.1 else '‚ö†Ô∏è Imbalanced'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metrics available to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Overall Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    print(\"ÔøΩÔøΩ OVERALL COMPARISON METRICS (HYBRID CONFIG)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simple LSTM metrics\n",
    "    print(\"\\nüîµ Simple LSTM Performance:\")\n",
    "    print(f\"  Total Loss: {metrics['simple_lstm_metrics']['total_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Loss: {metrics['simple_lstm_metrics']['magnitude_loss']:.4f}\")\n",
    "    print(f\"  Frequency Loss: {metrics['simple_lstm_metrics']['frequency_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Accuracy: {metrics['simple_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "    print(f\"  Frequency Accuracy: {metrics['simple_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "    print(f\"  Magnitude Correlation: {metrics['simple_lstm_metrics']['magnitude_corr']:.3f}\")\n",
    "    print(f\"  Frequency Correlation: {metrics['simple_lstm_metrics']['frequency_corr']:.3f}\")\n",
    "    \n",
    "    # Attention LSTM metrics\n",
    "    print(\"\\nüü° Attention LSTM Performance:\")\n",
    "    print(f\"  Total Loss: {metrics['attention_lstm_metrics']['total_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Loss: {metrics['attention_lstm_metrics']['magnitude_loss']:.4f}\")\n",
    "    print(f\"  Frequency Loss: {metrics['attention_lstm_metrics']['frequency_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Accuracy: {metrics['attention_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "    print(f\"  Frequency Accuracy: {metrics['attention_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "    print(f\"  Magnitude Correlation: {metrics['attention_lstm_metrics']['magnitude_corr']:.3f}\")\n",
    "    print(f\"  Frequency Correlation: {metrics['attention_lstm_metrics']['frequency_corr']:.3f}\")\n",
    "    \n",
    "    # Hyperparameters used\n",
    "    print(\"\\n‚öôÔ∏è  Hyperparameters Used:\")\n",
    "    for key, value in metrics['hyperparameters'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\nÔøΩÔøΩ PERFORMANCE COMPARISON:\")\n",
    "    \n",
    "    # Magnitude accuracy comparison\n",
    "    simple_mag_acc = metrics['simple_lstm_metrics']['magnitude_accuracy']\n",
    "    attention_mag_acc = metrics['attention_lstm_metrics']['magnitude_accuracy']\n",
    "    \n",
    "    if simple_mag_acc > attention_mag_acc:\n",
    "        improvement = ((simple_mag_acc - attention_mag_acc) / attention_mag_acc) * 100\n",
    "        print(f\"  üéØ Simple LSTM wins on Magnitude Accuracy: {simple_mag_acc:.3f} vs {attention_mag_acc:.3f} (+{improvement:.1f}%)\")\n",
    "    else:\n",
    "        improvement = ((attention_mag_acc - simple_mag_acc) / simple_mag_acc) * 100\n",
    "        print(f\"  üéØ Attention LSTM wins on Magnitude Accuracy: {attention_mag_acc:.3f} vs {simple_mag_acc:.3f} (+{improvement:.1f}%)\")\n",
    "    \n",
    "    # Frequency accuracy comparison\n",
    "    simple_freq_acc = metrics['simple_lstm_metrics']['frequency_accuracy']\n",
    "    attention_freq_acc = metrics['attention_lstm_metrics']['frequency_accuracy']\n",
    "    \n",
    "    if simple_freq_acc > attention_freq_acc:\n",
    "        improvement = ((simple_freq_acc - attention_freq_acc) / attention_freq_acc) * 100\n",
    "        print(f\"  üéØ Simple LSTM wins on Frequency Accuracy: {simple_freq_acc:.3f} vs {attention_freq_acc:.3f} (+{improvement:.1f}%)\")\n",
    "    else:\n",
    "        improvement = ((attention_freq_acc - simple_freq_acc) / simple_freq_acc) * 100\n",
    "        print(f\"  üéØ Attention LSTM wins on Frequency Accuracy: {attention_freq_acc:.3f} vs {simple_freq_acc:.3f} (+{improvement:.1f}%)\")\n",
    "    \n",
    "    # Overall winner\n",
    "    simple_total = simple_mag_acc + simple_freq_acc\n",
    "    attention_total = attention_mag_acc + attention_freq_acc\n",
    "    \n",
    "    if simple_total > attention_total:\n",
    "        print(f\"\\nüèÜ OVERALL WINNER: Simple LSTM ({simple_total:.3f} vs {attention_total:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\nüèÜ OVERALL WINNER: Attention LSTM ({attention_total:.3f} vs {simple_total:.3f})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metrics available to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hybrid Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    # Create comprehensive comparison plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    models = ['Simple LSTM', 'Attention LSTM']\n",
    "    mag_acc = [metrics['simple_lstm_metrics']['magnitude_accuracy'], \n",
    "               metrics['attention_lstm_metrics']['magnitude_accuracy']]\n",
    "    freq_acc = [metrics['simple_lstm_metrics']['frequency_accuracy'], \n",
    "                metrics['attention_lstm_metrics']['frequency_accuracy']]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, mag_acc, width, label='Magnitude Accuracy', alpha=0.8, color='skyblue')\n",
    "    bars2 = ax1.bar(x + width/2, freq_acc, width, label='Frequency Accuracy', alpha=0.8, color='lightcoral')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Hybrid Config: Accuracy Comparison', fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Loss Comparison\n",
    "    total_loss = [metrics['simple_lstm_metrics']['total_loss'], \n",
    "                  metrics['attention_lstm_metrics']['total_loss']]\n",
    "    mag_loss = [metrics['simple_lstm_metrics']['magnitude_loss'], \n",
    "                metrics['attention_lstm_metrics']['magnitude_loss']]\n",
    "    freq_loss = [metrics['simple_lstm_metrics']['frequency_loss'], \n",
    "                 metrics['attention_lstm_metrics']['frequency_loss']]\n",
    "    \n",
    "    ax2.bar(x - width/2, mag_loss, width, label='Magnitude Loss', alpha=0.8, color='lightblue')\n",
    "    ax2.bar(x + width/2, freq_loss, width, label='Frequency Loss', alpha=0.8, color='salmon')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Hybrid Config: Loss Comparison', fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correlation Comparison\n",
    "    mag_corr = [metrics['simple_lstm_metrics']['magnitude_corr'], \n",
    "                metrics['attention_lstm_metrics']['magnitude_corr']]\n",
    "    freq_corr = [metrics['simple_lstm_metrics']['frequency_corr'], \n",
    "                 metrics['attention_lstm_metrics']['frequency_corr']]\n",
    "    \n",
    "    ax3.bar(x - width/2, mag_corr, width, label='Magnitude Correlation', alpha=0.8, color='lightgreen')\n",
    "    ax3.bar(x + width/2, freq_corr, width, label='Frequency Correlation', alpha=0.8, color='orange')\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Correlation')\n",
    "    ax3.set_title('Hybrid Config: Correlation Comparison', fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(models)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Overall Performance Heatmap\n",
    "    performance_data = [\n",
    "        [mag_acc[0], freq_acc[0]],  # Simple LSTM\n",
    "        [mag_acc[1], freq_acc[1]]   # Attention LSTM\n",
    "    ]\n",
    "    \n",
    "    im = ax4.imshow(performance_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    ax4.set_xticks([0, 1])\n",
    "    ax4.set_xticklabels(['Magnitude', 'Frequency'])\n",
    "    ax4.set_yticks([0, 1])\n",
    "    ax4.set_yticklabels(['Simple LSTM', 'Attention LSTM'])\n",
    "    ax4.set_title('Hybrid Config: Performance Heatmap', fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = ax4.text(j, i, f'{performance_data[i][j]:.3f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Hybrid configuration visualizations created successfully!\")\n",
    "    print(\"\\nÔøΩÔøΩ Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Magnitude Performance: {'‚úÖ Good' if min(mag_acc) > 0.6 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "    print(f\"  ‚Ä¢ Frequency Performance: {'‚úÖ Good' if min(freq_acc) > 0.7 else '‚ö†Ô∏è Needs improvement'}\")\n",
    "    print(f\"  ‚Ä¢ Overfitting Prevention: {'‚úÖ Effective' if max(freq_acc) < 0.95 else '‚ö†Ô∏è May be overfitting'}\")\n",
    "    print(f\"  ‚Ä¢ Model Balance: {'‚úÖ Balanced' if abs(sum(mag_acc) - sum(freq_acc)) < 0.2 else '‚ö†Ô∏è Imbalanced'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Hybrid Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    # Create comprehensive summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Metric': ['Total Loss', 'Magnitude Loss', 'Frequency Loss', 'Magnitude Accuracy', \n",
    "                   'Frequency Accuracy', 'Magnitude Correlation', 'Frequency Correlation'],\n",
    "        'Simple_LSTM': [\n",
    "            metrics['simple_lstm_metrics']['total_loss'],\n",
    "            metrics['simple_lstm_metrics']['magnitude_loss'],\n",
    "            metrics['simple_lstm_metrics']['frequency_loss'],\n",
    "            metrics['simple_lstm_metrics']['magnitude_accuracy'],\n",
    "            metrics['simple_lstm_metrics']['frequency_accuracy'],\n",
    "            metrics['simple_lstm_metrics']['magnitude_corr'],\n",
    "            metrics['simple_lstm_metrics']['frequency_corr']\n",
    "        ],\n",
    "        'Attention_LSTM': [\n",
    "            metrics['attention_lstm_metrics']['total_loss'],\n",
    "            metrics['attention_lstm_metrics']['magnitude_loss'],\n",
    "            metrics['attention_lstm_metrics']['frequency_loss'],\n",
    "            metrics['attention_lstm_metrics']['magnitude_accuracy'],\n",
    "            metrics['attention_lstm_metrics']['frequency_accuracy'],\n",
    "            metrics['attention_lstm_metrics']['magnitude_corr'],\n",
    "            metrics['attention_lstm_metrics']['frequency_corr']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = \"hybrid_balanced_comparison_results.csv\"\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "    print(\"\\nÔøΩÔøΩ Results Summary:\")\n",
    "    print(summary_df.round(4))\n",
    "    \n",
    "    # Calculate differences and analysis\n",
    "    summary_df['Difference'] = summary_df['Attention_LSTM'] - summary_df['Simple_LSTM']\n",
    "    summary_df['Simple_LSTM_Wins'] = summary_df['Difference'] < 0\n",
    "    \n",
    "    print(\"\\nüìä Performance Differences (Attention - Simple):\")\n",
    "    print(summary_df[['Metric', 'Difference']].round(4))\n",
    "    \n",
    "    # Count wins\n",
    "    simple_wins = summary_df['Simple_LSTM_Wins'].sum()\n",
    "    attention_wins = len(summary_df) - simple_wins\n",
    "    \n",
    "    print(f\"\\nüèÜ Final Score:\")\n",
    "    print(f\"  Simple LSTM wins: {simple_wins} metrics\")\n",
    "    print(f\"  Attention LSTM wins: {attention_wins} metrics\")\n",
    "    \n",
    "    if simple_wins > attention_wins:\n",
    "        print(f\"  üéØ OVERALL WINNER: Simple LSTM\")\n",
    "    elif attention_wins > simple_wins:\n",
    "        print(f\"  üéØ OVERALL WINNER: Attention LSTM\")\n",
    "    else:\n",
    "        print(f\"  üéØ TIE: Both models perform equally well\")\n",
    "    \n",
    "    # Hybrid config assessment\n",
    "    print(f\"\\nüî¨ HYBRID CONFIG ASSESSMENT:\")\n",
    "    \n",
    "    # Check if hybrid config achieved its goals\n",
    "    mag_performance = min(metrics['simple_lstm_metrics']['magnitude_accuracy'], \n",
    "                          metrics['attention_lstm_metrics']['magnitude_accuracy'])\n",
    "    freq_performance = min(metrics['simple_lstm_metrics']['frequency_accuracy'], \n",
    "                           metrics['attention_lstm_metrics']['frequency_accuracy'])\n",
    "    max_freq = max(metrics['simple_lstm_metrics']['frequency_accuracy'], \n",
    "                   metrics['attention_lstm_metrics']['frequency_accuracy'])\n",
    "    \n",
    "    print(f\"  Magnitude Target (70-85%): {'‚úÖ Achieved' if 0.7 <= mag_performance <= 0.85 else '‚ö†Ô∏è Outside target'}\")\n",
    "    print(f\"  Frequency Target (75-90%): {'‚úÖ Achieved' if 0.75 <= freq_performance <= 0.9 else '‚ö†Ô∏è Outside target'}\")\n",
    "    print(f\"  Overfitting Prevention: {'‚úÖ Effective' if max_freq < 0.95 else '‚ö†Ô∏è May be overfitting'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metrics available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    print(\"ÔøΩÔøΩ HYBRID CONFIG COMPREHENSIVE SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Configuration: Hybrid Balanced Config\")\n",
    "    print(f\"Analysis Type: Anti-Overfitting + High Performance Hybrid\")\n",
    "    print()\n",
    "    \n",
    "    # Overall performance\n",
    "    print(\"üèÜ OVERALL PERFORMANCE:\")\n",
    "    simple_mag_acc = metrics['simple_lstm_metrics']['magnitude_accuracy']\n",
    "    simple_freq_acc = metrics['simple_lstm_metrics']['frequency_accuracy']\n",
    "    attention_mag_acc = metrics['attention_lstm_metrics']['magnitude_accuracy']\n",
    "    attention_freq_acc = metrics['attention_lstm_metrics']['frequency_accuracy']\n",
    "    \n",
    "    print(f\"  Simple LSTM:     {simple_mag_acc:.3f} magnitude, {simple_freq_acc:.3f} frequency\")\n",
    "    print(f\"  Attention LSTM:  {attention_mag_acc:.3f} magnitude, {attention_freq_acc:.3f} frequency\")\n",
    "    print()\n",
    "    \n",
    "    # Winner determination\n",
    "    simple_total = simple_mag_acc + simple_freq_acc\n",
    "    attention_total = attention_mag_acc + attention_freq_acc\n",
    "    \n",
    "    if attention_total > simple_total:\n",
    "        print(\"üéØ WINNER: Attention LSTM\")\n",
    "        print(f\"   Reason: Higher combined accuracy ({attention_total:.3f} vs {simple_total:.3f})\")\n",
    "    else:\n",
    "        print(\"üéØ WINNER: Simple LSTM\")\n",
    "        print(f\"   Reason: Higher combined accuracy ({simple_total:.3f} vs {attention_total:.3f})\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Hybrid config effectiveness\n",
    "    print(\"üî¨ HYBRID CONFIG EFFECTIVENESS:\")\n",
    "    max_acc = max(simple_mag_acc, simple_freq_acc, attention_mag_acc, attention_freq_acc)\n",
    "    \n",
    "    # Check anti-overfitting success\n",
    "    if max_acc < 0.95:\n",
    "        print(f\"  ‚úÖ Anti-Overfitting: Effective (max accuracy {max_acc:.3f} < 0.95)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Anti-Overfitting: May need adjustment (max accuracy {max_acc:.3f} >= 0.95)\")\n",
    "    \n",
    "    # Check performance balance\n",
    "    mag_range = max(simple_mag_acc, attention_mag_acc) - min(simple_mag_acc, attention_mag_acc)\n",
    "    freq_range = max(simple_freq_acc, attention_freq_acc) - min(simple_freq_acc, attention_freq_acc)\n",
    "    \n",
    "    print(f\"  Magnitude consistency: {'‚úÖ Good' if mag_range < 0.2 else '‚ö†Ô∏è High variance'}\")\n",
    "    print(f\"  Frequency consistency: {'‚úÖ Good' if freq_range < 0.2 else '‚ö†Ô∏è High variance'}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"ÔøΩÔøΩ RECOMMENDATIONS:\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    if attention_total > simple_total + 0.1:\n",
    "        print(\"  ‚Ä¢ Attention LSTM shows significant improvement - consider for production\")\n",
    "    elif simple_total > attention_total + 0.1:\n",
    "        print(\"  ‚Ä¢ Simple LSTM performs better - simpler model may be sufficient\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Both models perform similarly - choose based on computational requirements\")\n",
    "    \n",
    "    # Config adjustment recommendations\n",
    "    if max_acc < 0.8:\n",
    "        print(\"  ‚Ä¢ Hybrid config is working well - realistic performance achieved\")\n",
    "    elif max_acc < 0.9:\n",
    "        print(\"  ‚Ä¢ Hybrid config is balanced - good performance without overfitting\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Consider reducing model capacity or increasing regularization\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "    print(f\"  1. Run per-bin analysis to get spatial performance details\")\n",
    "    print(f\"  2. Test on validation set to confirm generalization\")\n",
    "    print(f\"  3. Deploy winning model to production\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üéØ Hybrid configuration analysis completed successfully!\")\n",
    "    print(\"\\nNote: This analysis shows overall model performance.\")\n",
    "    print(\"For per-bin accuracy analysis, you would need to run the trained models\")\n",
    "    print(\"on test data with spatial binning to get detailed per-bin metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "ipykernel": {
    "display_name": "Python 3",
    "language": "python"
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}