#!/usr/bin/env python3
"""
Earthquake Preprocessing Module

Implements the rule-based algorithm for:
1. Filtering shallow earthquakes (<70km depth)
2. Classifying earthquakes into quadtree bins
3. Computing annual statistics per bin
4. Preparing data for LSTM training
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Tuple, Dict, List
import logging
from pathlib import Path

# Add src to path for imports
import sys
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from src.binning.quadtree import QuadtreeBinner


class EarthquakeProcessor:
    """
    Earthquake data processor implementing the methodology from the paper.
    
    Features:
    - Filters shallow earthquakes (<70km depth)
    - Classifies earthquakes into quadtree bins
    - Computes annual statistics (frequency, max magnitude)
    - Prepares data for LSTM training with 10-year lookback
    """
    
    def __init__(self, min_depth: float = 70.0):
        """Initialize the EarthquakeProcessor."""
        self.min_depth = min_depth
        # Adaptive quadtree parameters for proper spatial binning
        # Custom bounds to include bottom-right bin with 10 earthquakes and allow left edge merging
        # Expanded boundaries for better coverage and merging
        custom_bounds = (116.3, 129.0, 2.0, 22.0)  # (min_lon, max_lon, min_lat, max_lat)
        
        self.quadtree_binner = QuadtreeBinner(
            max_depth=3,           # Deeper tree for more granular initial subdivision
            min_events=100,        # Higher minimum for adaptive binning
            merge_threshold=50,    # Target threshold for merging
            max_bin_size=40.0,     # Even larger max size to eliminate remaining low-count bins
            custom_bounds=custom_bounds  # Focus on main Philippines region
        )
        self.logger = logging.getLogger(__name__)
        
        # Column mapping for different naming conventions
        self.column_mapping = {
            'depth': ['depth', 'Depth', 'DEPTH'],
            'latitude': ['latitude', 'lat', 'Lat', 'LAT', 'N_Lat'],
            'longitude': ['longitude', 'lon', 'Lon', 'LON', 'E_Long'],
            'magnitude': ['magnitude', 'mag', 'Mag', 'MAG', 'Mag'],
            'year': ['year', 'Year', 'YEAR'],
            'month': ['month', 'Month', 'MONTH'],
            'day': ['day', 'Day', 'DAY']
        }
    
    def _get_column_name(self, df: pd.DataFrame, target_col: str) -> str:
        """
        Get the actual column name from the DataFrame based on target column.
        
        Args:
            df: DataFrame to search in
            target_col: Target column type (e.g., 'depth', 'latitude')
            
        Returns:
            Actual column name found in DataFrame
        """
        if target_col not in self.column_mapping:
            raise ValueError(f"Unknown target column: {target_col}")
        
        possible_names = self.column_mapping[target_col]
        for name in possible_names:
            if name in df.columns:
                return name
        
        # If not found, show available columns for debugging
        available_cols = list(df.columns)
        raise ValueError(f"Column '{target_col}' not found. Available columns: {available_cols}")
        
    def filter_shallow_earthquakes(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Filter earthquakes to only include shallow ones (<70km depth).
        
        Args:
            df: DataFrame with earthquake data
            
        Returns:
            Filtered DataFrame with only shallow earthquakes
        """
        self.logger.info(f"Filtering earthquakes with depth < {self.min_depth}km")
        
        # Get the actual depth column name
        depth_col = self._get_column_name(df, 'depth')
        
        # Filter by depth
        shallow_df = df[df[depth_col] < self.min_depth].copy()
        
        self.logger.info(f"Original earthquakes: {len(df)}")
        self.logger.info(f"Shallow earthquakes (<{self.min_depth}km): {len(shallow_df)}")
        
        return shallow_df
    
    def classify_quadtree_bins(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Classify earthquakes into quadtree bins based on latitude/longitude.
        
        Args:
            df: DataFrame with earthquake data (must have 'latitude' and 'longitude' columns)
            
        Returns:
            DataFrame with additional 'bin_id' column
        """
        self.logger.info("Classifying earthquakes into quadtree bins")
        
        # Get the actual column names
        lat_col = self._get_column_name(df, 'latitude')
        lon_col = self._get_column_name(df, 'longitude')
        
        # Create quadtree bins
        df_with_bins = df.copy()
        # Use non-overlapping bins for better spatial separation
        bin_ids = self.quadtree_binner.assign_bins(
            df[lat_col].values, 
            df[lon_col].values
        )
        df_with_bins['bin_id'] = bin_ids
        
        # Debug: Check bin ID distribution
        unique_bins = df_with_bins['bin_id'].nunique()
        bin_counts = df_with_bins['bin_id'].value_counts().sort_index()
        self.logger.info(f"Created {unique_bins} quadtree bins")
        self.logger.info(f"Bin ID distribution: {dict(bin_counts)}")
        self.logger.info(f"Bin IDs range: {bin_ids.min()} to {bin_ids.max()}")
        
        return df_with_bins
    
    def compute_annual_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Compute annual statistics for each quadtree bin.
        
        Args:
            df: DataFrame with earthquake data and bin_id
            
        Returns:
            DataFrame with annual statistics per bin
        """
        self.logger.info("Computing annual statistics per quadtree bin")
        
        # Ensure we have required columns
        if 'bin_id' not in df.columns:
            raise ValueError("DataFrame must have 'bin_id' column from classify_quadtree_bins")
        
        # Extract year from Date_Time column first
        if 'Date_Time' in df.columns:
            df['year'] = df['Date_Time'].dt.year
        elif 'Year' in df.columns and 'Month' in df.columns and 'Day' in df.columns:
            df['year'] = df['Year']
        
        # Now get the actual column names (year should exist now)
        mag_col = self._get_column_name(df, 'magnitude')
        depth_col = self._get_column_name(df, 'depth')
        
        # Group by year and bin_id, compute statistics
        annual_stats = df.groupby(['year', 'bin_id']).agg({
            mag_col: ['max', 'mean'],
            depth_col: 'mean'
        }).reset_index()
        
        # Flatten column names
        annual_stats.columns = ['year', 'bin_id', 'max_magnitude', 'avg_magnitude', 'avg_depth']
        
        # Add frequency (count of earthquakes per year per bin)
        frequency = df.groupby(['year', 'bin_id']).size().reset_index(name='frequency')
        annual_stats = annual_stats.merge(frequency, on=['year', 'bin_id'])
        
        # Sort by year and bin_id
        annual_stats = annual_stats.sort_values(['year', 'bin_id']).reset_index(drop=True)
        
        self.logger.info(f"Computed annual statistics for {len(annual_stats)} year-bin combinations")
        
        return annual_stats
    
    def prepare_lstm_data(self, annual_stats: pd.DataFrame, lookback_years: int = 10) -> pd.DataFrame:
        """
        Prepare data for LSTM training with sliding window approach.
        
        Args:
            annual_stats: DataFrame with annual statistics per bin
            lookback_years: Number of years to look back (default: 10)
            
        Returns:
            DataFrame ready for LSTM training
        """
        self.logger.info(f"Preparing LSTM data with {lookback_years}-year lookback")
        
        # Get unique bins
        unique_bins = annual_stats['bin_id'].unique()
        self.logger.info(f"Preparing data for {len(unique_bins)} quadtree bins")
        
        lstm_data = []
        
        for bin_id in unique_bins:
            bin_data = annual_stats[annual_stats['bin_id'] == bin_id].copy()
            bin_data = bin_data.sort_values('year').reset_index(drop=True)
            
            # Create sliding windows
            for i in range(len(bin_data) - lookback_years):
                # Input sequence (lookback years)
                input_sequence = bin_data.iloc[i:i+lookback_years]
                
                # Target (next year)
                target = bin_data.iloc[i+lookback_years]
                
                # Create features for each year in sequence
                sequence_features = []
                for _, row in input_sequence.iterrows():
                    features = {
                        'bin_id': bin_id,
                        'year': row['year'],
                        'max_magnitude': row['max_magnitude'],
                        'frequency': row['frequency']
                    }
                    sequence_features.append(features)
                
                # Add target information
                lstm_data.append({
                    'bin_id': bin_id,
                    'input_sequence': sequence_features,
                    'target_year': target['year'],
                    'target_max_magnitude': target['max_magnitude'],
                    'target_frequency': target['frequency']
                })
        
        self.logger.info(f"Created {len(lstm_data)} LSTM training samples")
        return lstm_data
    
    def process_catalog(self, df: pd.DataFrame, save_path: str = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Complete processing pipeline for earthquake catalog.
        
        Args:
            df: Raw earthquake catalog DataFrame
            save_path: Optional path to save processed data
            
        Returns:
            Tuple of (processed_catalog, annual_statistics)
        """
        self.logger.info("Starting complete earthquake catalog processing")
        
        # Step 1: Filter shallow earthquakes
        shallow_df = self.filter_shallow_earthquakes(df)
        
        # Step 2: Classify into quadtree bins
        binned_df = self.classify_quadtree_bins(shallow_df)
        
        # Step 3: Standardize column names for processed data
        binned_df = self._standardize_columns(binned_df)
        
        # Step 4: Compute annual statistics
        annual_stats = self.compute_annual_statistics(binned_df)
        
        # Step 5: Save processed data if path provided
        if save_path:
            self.save_processed_data(binned_df, annual_stats, save_path)
            
            # Step 6: Create and save quadtree visualizations
            try:
                # Create regular quadtree visualization
                plot_path = Path(save_path).parent / "quadtree_bins_visualization.png"
                self.plot_quadtree_bins(binned_df, str(plot_path))
                self.logger.info("Generated quadtree bins visualization")
                
                # Create comparison visualization (unmerged vs merged)
                comparison_path = Path(save_path).parent / "quadtree_comparison.png"
                self.plot_quadtree_comparison(binned_df, str(comparison_path))
                self.logger.info("Generated quadtree comparison visualization")
                
            except Exception as e:
                self.logger.warning(f"Could not generate visualizations: {e}")
        
        return binned_df, annual_stats
    
    def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Standardize column names for processed data.
        
        Args:
            df: DataFrame with original column names
            
        Returns:
            DataFrame with standardized column names
        """
        df_std = df.copy()
        
        # Map original column names to standard names
        column_mapping = {}
        
        # Map depth column
        depth_col = self._get_column_name(df, 'depth')
        if depth_col != 'depth':
            column_mapping[depth_col] = 'depth'
        
        # Map magnitude column
        mag_col = self._get_column_name(df, 'magnitude')
        if mag_col != 'magnitude':
            column_mapping[mag_col] = 'magnitude'
        
        # Map latitude column
        lat_col = self._get_column_name(df, 'latitude')
        if lat_col != 'latitude':
            column_mapping[lat_col] = 'latitude'
        
        # Map longitude column
        lon_col = self._get_column_name(df, 'longitude')
        if lon_col != 'longitude':
            column_mapping[lon_col] = 'longitude'
        
        # Rename columns if any mapping exists
        if column_mapping:
            df_std = df_std.rename(columns=column_mapping)
            self.logger.info(f"Standardized column names: {column_mapping}")
        
        return df_std
    
    def save_processed_data(self, processed_catalog: pd.DataFrame, 
                           annual_stats: pd.DataFrame, base_path: str):
        """
        Save processed data to CSV files.
        
        Args:
            processed_catalog: Processed earthquake catalog with bins
            annual_stats: Annual statistics per bin
            base_path: Base path for saving files
        """
        # Create directory if it doesn't exist
        save_dir = Path(base_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # Save processed catalog
        catalog_path = Path(base_path)
        processed_catalog.to_csv(catalog_path, index=False)
        self.logger.info(f"Saved processed catalog to: {catalog_path}")
        
        # Save annual statistics
        stats_path = catalog_path.parent / f"{catalog_path.stem}_annual_stats.csv"
        annual_stats.to_csv(stats_path, index=False)
        self.logger.info(f"Saved annual statistics to: {stats_path}")
        
        # Save LSTM-ready data
        lstm_data = self.prepare_lstm_data(annual_stats)
        lstm_path = catalog_path.parent / f"{catalog_path.stem}_lstm_ready.csv"
        
        # Convert LSTM data to DataFrame format
        lstm_df = []
        for item in lstm_data:
            for seq_item in item['input_sequence']:
                lstm_df.append({
                    'bin_id': seq_item['bin_id'],
                    'year': seq_item['year'],
                    'max_magnitude': seq_item['max_magnitude'],
                    'frequency': seq_item['frequency'],
                    'target_year': item['target_year'],
                    'target_max_magnitude': item['target_max_magnitude'],
                    'target_frequency': item['target_frequency']
                })
        
        lstm_df = pd.DataFrame(lstm_df)
        lstm_df.to_csv(lstm_path, index=False)
        self.logger.info(f"Saved LSTM-ready data to: {lstm_path}")
    
    def get_bin_statistics(self, annual_stats: pd.DataFrame) -> pd.DataFrame:
        """
        Get summary statistics for each quadtree bin.
        
        Args:
            annual_stats: Annual statistics DataFrame
            
        Returns:
            Summary statistics per bin
        """
        bin_summary = annual_stats.groupby('bin_id').agg({
            'max_magnitude': ['mean', 'std', 'min', 'max'],
            'frequency': ['mean', 'std', 'min', 'max'],
            'year': ['min', 'max', 'count']
        }).round(3)
        
        # Flatten column names
        bin_summary.columns = ['_'.join(col).strip() for col in bin_summary.columns]
        bin_summary = bin_summary.reset_index()
        
        return bin_summary
    
    def plot_quadtree_bins(self, processed_catalog: pd.DataFrame, save_path: str = None):
        """
        Plot quadtree bins exactly like your notebook - with adaptive grid and event markers.
        
        Args:
            processed_catalog: Processed earthquake catalog with bin_id
            save_path: Optional path to save the plot
        """
        print("ðŸŽ¨ Starting quadtree bins plotting like your notebook...")
        try:
            import matplotlib.pyplot as plt
            import matplotlib.patches as patches
            import cartopy.crs as ccrs
            import cartopy.feature as cfeature
            
            print("âœ… Cartopy imports successful")
            
            # Create figure with cartopy projection (exactly like your notebook)
            fig = plt.figure(figsize=(12, 10))
            ax = plt.axes(projection=ccrs.PlateCarree())
            
            # Set extent exactly like your notebook
            ax.set_extent([116.3, 133.0, 2.0, 22.0], crs=ccrs.PlateCarree())
            
            # Add map features exactly like your notebook
            ax.coastlines(resolution='10m')
            ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)
            ax.add_feature(cfeature.LAND, facecolor='lightgray', edgecolor='black', alpha=0.3)
            
            # Get earthquake coordinates for plotting
            lons = processed_catalog['longitude'].values
            lats = processed_catalog['latitude'].values
            magnitudes = processed_catalog['magnitude'].values
            
            # Plot all earthquake events as blue dots (like your notebook)
            ax.scatter(lons, lats, s=10, color='blue', alpha=0.3, 
                      label='All Events', transform=ccrs.PlateCarree())
            
            # Plot filtered earthquakes (those with bin_id >= 0) as red circles
            filtered_mask = processed_catalog['bin_id'] >= 0
            if filtered_mask.any():
                filtered_lons = processed_catalog.loc[filtered_mask, 'longitude'].values
                filtered_lats = processed_catalog.loc[filtered_mask, 'latitude'].values
                filtered_mags = processed_catalog.loc[filtered_mask, 'magnitude'].values
                
                ax.scatter(filtered_lons, filtered_lats, s=filtered_mags**2, 
                          color='red', alpha=0.6, label='Filtered Earthquakes',
                          transform=ccrs.PlateCarree())
            
            # Plot quadtree grid exactly like your notebook
            if hasattr(self.quadtree_binner, 'bounds') and self.quadtree_binner.bounds:
                bounds = self.quadtree_binner.bounds
                print(f"ðŸ“ Plotting {len(bounds)} quadtree bins...")
                
                for i, (min_lon, max_lon, min_lat, max_lat) in enumerate(bounds):
                    # Create rectangle for this bin
                    rect = plt.Rectangle((min_lon, min_lat), max_lon - min_lon, max_lat - min_lat,
                                       edgecolor='black', facecolor='none', linewidth=1,
                                       transform=ccrs.PlateCarree())
                    ax.add_patch(rect)
                    
                    # Count events in this bin
                    mask = ((lons >= min_lon) & (lons < max_lon) & 
                           (lats >= min_lat) & (lats < max_lat))
                    count = np.sum(mask)
                    
                    # Add count label in center of bin (like your notebook)
                    center_lon = (min_lon + max_lon) / 2
                    center_lat = (min_lat + max_lat) / 2
                    ax.text(center_lon, center_lat, str(count), 
                           ha='center', va='center', fontsize=10, fontweight='bold',
                           transform=ccrs.PlateCarree(),
                           bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.9))
            
            # Add title exactly like your notebook
            ax.set_title(f"Quadtree Grid (Merged bins < 50 events)")
            
            # Add legend exactly like your notebook
            ax.legend()
            
            # Tight layout
            plt.tight_layout()
            
            print("ðŸŽ¨ Plot created successfully, showing...")
            
            # Save plot if path provided
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ðŸ’¾ Plot saved to: {save_path}")
            else:
                default_save_path = "data/quadtree_grid_notebook_style.png"
                plt.savefig(default_save_path, dpi=300, bbox_inches='tight')
                print(f"ðŸ’¾ Plot saved to: {default_save_path}")
            
            # Show plot
            plt.show()
            
            print("âœ… Plotting completed successfully!")
            
        except ImportError as e:
            self.logger.warning(f"Cartopy not available for plotting: {e}")
            print("âš ï¸ Cartopy not available, falling back to simple matplotlib...")
            self._plot_quadtree_bins_simple(processed_catalog, save_path)
        except Exception as e:
            self.logger.error(f"Error creating quadtree visualization: {e}")
            raise
    
    def plot_quadtree_comparison(self, processed_catalog: pd.DataFrame, save_path: str = None):
        """
        Plot both unmerged and merged quadtree grids for comparison.
        
        Args:
            processed_catalog: Processed earthquake catalog with bin_id
            save_path: Optional path to save the plot
        """
        print("ðŸŽ¨ Creating quadtree comparison plot (unmerged vs merged)...")
        
        if not hasattr(self.quadtree_binner, 'unmerged_bounds') or not hasattr(self.quadtree_binner, 'bounds'):
            print("âŒ No quadtree bounds available. Run classify_quadtree_bins() first.")
            return
        
        try:
            import matplotlib.pyplot as plt
            import matplotlib.patches as patches
            import cartopy.crs as ccrs
            import cartopy.feature as cfeature
            
            print("âœ… Cartopy imports successful")
            
            # Get bounds
            unmerged_bounds = self.quadtree_binner.get_unmerged_bounds()
            merged_bounds = self.quadtree_binner.get_bin_bounds()
            
            print(f"ðŸ“Š Unmerged bins: {len(unmerged_bounds)}")
            print(f"ðŸ“Š Merged bins: {len(merged_bounds)}")
            
            # Create figure with two subplots
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), 
                                           subplot_kw={'projection': ccrs.PlateCarree()})
            
            # Set extent for both plots
            lons = processed_catalog['longitude'].values
            lats = processed_catalog['latitude'].values
            min_lon, max_lon = np.min(lons), np.max(lons)
            min_lat, max_lat = np.min(lats), np.max(lats)
            
            # Add some padding
            lon_pad = (max_lon - min_lon) * 0.05
            lat_pad = (max_lat - min_lat) * 0.05
            
            extent = [min_lon - lon_pad, max_lon + lon_pad, 
                      min_lat - lat_pad, max_lat + lat_pad]
            
            for ax in [ax1, ax2]:
                ax.set_extent(extent, crs=ccrs.PlateCarree())
                ax.coastlines(resolution='10m')
                ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)
                ax.add_feature(cfeature.LAND, facecolor='lightgray', edgecolor='black', alpha=0.3)
            
            # Plot 1: Unmerged quadtree
            ax1.scatter(lons, lats, s=10, color='blue', alpha=0.3, 
                        label='All Events', transform=ccrs.PlateCarree())
            
            # Draw unmerged cell outlines
            for i, (min_lon, max_lon, min_lat, max_lat) in enumerate(unmerged_bounds):
                rect = plt.Rectangle((min_lon, min_lat), max_lon - min_lon, max_lat - min_lat,
                                   edgecolor='red', facecolor='none', linewidth=1.5,
                                   transform=ccrs.PlateCarree())
                ax1.add_patch(rect)
                
                # Add count label
                mask = ((lons >= min_lon) & (lons < max_lon) & 
                       (lats >= min_lat) & (lats < max_lat))
                count = np.sum(mask)
                center_lon = (min_lon + max_lon) / 2
                center_lat = (min_lat + max_lat) / 2
                ax1.text(center_lon, center_lat, str(count), 
                        ha='center', va='center', fontsize=8, fontweight='bold',
                        transform=ccrs.PlateCarree(),
                        bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
            
            ax1.set_title(f"Unmerged Quadtree ({len(unmerged_bounds)} bins)", fontsize=14, fontweight='bold')
            ax1.legend()
            
            # Plot 2: Merged quadtree
            ax2.scatter(lons, lats, s=10, color='blue', alpha=0.3, 
                        label='All Events', transform=ccrs.PlateCarree())
            
            # Draw merged cell outlines
            for i, (min_lon, max_lon, min_lat, max_lat) in enumerate(merged_bounds):
                rect = plt.Rectangle((min_lon, min_lat), max_lon - min_lon, max_lat - min_lat,
                                   edgecolor='green', facecolor='none', linewidth=2,
                                   transform=ccrs.PlateCarree())
                ax2.add_patch(rect)
                
                # Add count label
                mask = ((lons >= min_lon) & (lons < max_lon) & 
                       (lats >= min_lat) & (lats < max_lat))
                count = np.sum(mask)
                center_lon = (min_lon + max_lon) / 2
                center_lat = (min_lat + max_lat) / 2
                ax2.text(center_lon, center_lat, str(count), 
                        ha='center', va='center', fontsize=8, fontweight='bold',
                        transform=ccrs.PlateCarree(),
                        bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
            
            merge_title = f"Merged Quadtree ({len(merged_bounds)} bins)"
            if hasattr(self.quadtree_binner, 'merge_threshold') and self.quadtree_binner.merge_threshold:
                merge_title += f"\nThreshold: <{self.quadtree_binner.merge_threshold} events"
            ax2.set_title(merge_title, fontsize=14, fontweight='bold')
            ax2.legend()
            
            # Add overall title
            fig.suptitle("Quadtree Binning Comparison: Unmerged vs Merged", 
                         fontsize=16, fontweight='bold', y=0.95)
            
            plt.tight_layout()
            
            # Save plot if path provided
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ðŸ’¾ Comparison plot saved to: {save_path}")
            else:
                default_save_path = "data/quadtree_comparison.png"
                plt.savefig(default_save_path, dpi=300, bbox_inches='tight')
                print(f"ðŸ’¾ Comparison plot saved to: {default_save_path}")
            
            # Show plot
            plt.show()
            
            print("âœ… Comparison plot completed!")
            
        except ImportError as e:
            self.logger.warning(f"Cartopy not available for plotting: {e}")
            print("âš ï¸ Cartopy not available, cannot create comparison plot")
        except Exception as e:
            self.logger.error(f"Error creating quadtree comparison: {e}")
            raise
    
    def show_quadtree_bins(self, processed_catalog: pd.DataFrame):
        """
        Display quadtree bins visualization without saving.
        
        Args:
            processed_catalog: Processed earthquake catalog with bin_id
        """
        self.plot_quadtree_bins(processed_catalog)
    
    def _plot_quadtree_bins_simple(self, processed_catalog: pd.DataFrame, save_path: str = None):
        """Fallback simple matplotlib plotting when cartopy is not available."""
        print("âš ï¸ Using simple matplotlib fallback...")
        # This would be the simple matplotlib version
        pass
