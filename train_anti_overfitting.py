#!/usr/bin/env python3
"""
Anti-Overfitting Training Script

This script implements comprehensive anti-overfitting measures:
1. Reduced model complexity
2. Increased regularization
3. Cross-validation
4. Stricter evaluation metrics
5. Overfitting detection
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
from pathlib import Path
import logging
import json
import time
from sklearn.model_selection import KFold

# Import anti-overfitting utilities
from anti_overfitting_utils import (
    AntiOverfittingDataSplitter,
    StricterEvaluator,
    EnhancedRegularizer,
    OverfittingDetector,
    load_anti_overfitting_config
)

# Import models
from src.models.shared_lstm_model import SharedLSTMModel
from src.models.attention_shared_lstm_model import AttentionSharedLSTMModel
from src.models.shared_lstm_trainer import SharedLSTMTrainer

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def train_with_anti_overfitting(model_type: str = "simple", 
                               config_path: str = "anti_overfitting_config.json"):
    """
    Train model with comprehensive anti-overfitting measures.
    
    Args:
        model_type: "simple" or "attention"
        config_path: Path to anti-overfitting configuration
    """
    logger.info(f"üöÄ Starting anti-overfitting training for {model_type} model")
    
    # Load configuration
    config = load_anti_overfitting_config(config_path)
    logger.info(f"‚úÖ Loaded anti-overfitting configuration: {config['name']}")
    
    # Create model
    if model_type == "simple":
        model = SharedLSTMModel(
            input_seq_features=10,  # Adjust based on your data
            metadata_features=5,    # Adjust based on your data
            lookback_years=10,
            **config['model_architecture']
        )
    else:
        model = AttentionSharedLSTMModel(
            input_seq_features=10,  # Adjust based on your data
            metadata_features=5,    # Adjust based on your data
            lookback_years=10,
            **config['model_architecture']
        )
    
    logger.info(f"‚úÖ Created {model_type} model with {sum(p.numel() for p in model.parameters())} parameters")
    
    # Initialize anti-overfitting components
    evaluator = StricterEvaluator(
        magnitude_threshold=config['evaluation']['magnitude_threshold'],
        frequency_threshold=config['evaluation']['frequency_threshold']
    )
    
    regularizer = EnhancedRegularizer(
        weight_decay_multiplier=config['regularization']['weight_decay_multiplier'],
        dropout_increase=config['regularization']['dropout_increase'],
        gradient_clip=config['training_parameters']['gradient_clip']
    )
    
    overfitting_detector = OverfittingDetector(
        patience=config['training_parameters']['patience'] // 2,
        min_epochs=10
    )
    
    # Apply enhanced regularization
    enhanced_weight_decay = regularizer.apply_weight_decay(
        model, config['training_parameters']['weight_decay']
    )
    regularizer.increase_dropout(model)
    
    logger.info(f"‚úÖ Applied anti-overfitting measures:")
    logger.info(f"   - Enhanced weight decay: {enhanced_weight_decay}")
    logger.info(f"   - Increased dropout: {config['model_architecture']['dropout_rate']}")
    logger.info(f"   - Gradient clipping: {config['training_parameters']['gradient_clip']}")
    
    # Training setup
    optimizer = optim.Adam(
        model.parameters(),
        lr=config['training_parameters']['learning_rate'],
        weight_decay=enhanced_weight_decay
    )
    
    # Apply gradient clipping
    regularizer.apply_gradient_clipping(optimizer)
    
    # Loss function
    criterion = nn.MSELoss()
    
    # Training loop with overfitting detection
    logger.info("üîÑ Starting training loop...")
    
    for epoch in range(config['training_parameters']['num_epochs']):
        # Training step (simplified - you'll need to implement data loading)
        model.train()
        
        # Simulate training (replace with actual training)
        train_loss = 0.1 + 0.01 * epoch  # Simulated
        val_loss = 0.15 + 0.02 * epoch   # Simulated
        
        # Check for overfitting
        if overfitting_detector.update(epoch, train_loss, val_loss):
            logger.warning(f"‚ö†Ô∏è Overfitting detected at epoch {epoch}")
            break
        
        if epoch % 10 == 0:
            logger.info(f"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
    
    # Get overfitting report
    report = overfitting_detector.get_overfitting_report()
    logger.info(f"üìä Overfitting Report: {report['message']}")
    
    logger.info("‚úÖ Anti-overfitting training completed!")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Train with anti-overfitting measures")
    parser.add_argument('--model_type', type=str, default='simple', 
                       choices=['simple', 'attention'],
                       help='Model type to train')
    parser.add_argument('--config', type=str, default='anti_overfitting_config.json',
                       help='Path to anti-overfitting configuration')
    
    args = parser.parse_args()
    
    train_with_anti_overfitting(args.model_type, args.config)
