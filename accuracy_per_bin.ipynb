{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-Bin Accuracy Analysis with Anti-Overfitting Config\n",
    "\n",
    "This notebook analyzes the per-bin accuracy for both LSTM and Attention LSTM models using the anti-overfitting configuration to prevent overfitting.\n",
    "\n",
    "## Overview\n",
    "- Loads existing trained models from anti_overfitting comparison\n",
    "- Evaluates performance across quadtree bins\n",
    "- Compares forecast accuracy, WMAPE, and other metrics per bin\n",
    "- Visualizes results with heatmaps and comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Anti-Overfitting Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anti-overfitting configuration\n",
    "with open('anti_overfitting_config.json', 'r') as f:\n",
    "    anti_overfitting_config = json.load(f)\n",
    "\n",
    "print(\"Anti-Overfitting Configuration:\")\n",
    "print(f\"Name: {anti_overfitting_config['name']}\")\n",
    "print(f\"Description: {anti_overfitting_config['description']}\")\n",
    "print(f\"Version: {anti_overfitting_config['version']}\")\n",
    "print()\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "for key, value in anti_overfitting_config['model_architecture'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"Training Parameters:\")\n",
    "for key, value in anti_overfitting_config['training_parameters'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"Anti-Overfitting Features:\")\n",
    "for key, value in anti_overfitting_config['anti_overfitting_features'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "print(\"Expected Behavior:\")\n",
    "for key, value in anti_overfitting_config['expected_behavior'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Output Directory (Use Existing Trained Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use existing trained models - skip training\n",
    "output_dir = \"data/results_anti_overfitting_comparison\"\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Full path: {Path(output_dir).absolute()}\")\n",
    "print(f\"Results dir: {Path(output_dir) / 'results' / 'model_comparison'}\")\n",
    "print(f\"Results dir exists: {(Path(output_dir) / 'results' / 'model_comparison').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comparison_results(output_dir: str) -> Dict:\n",
    "    \"\"\"Load the comparison results from the output directory.\"\"\"\n",
    "    \n",
    "    results_dir = Path(output_dir) / \"results\" / \"model_comparison\"\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        print(f\"‚ùå Results directory not found: {results_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Found results directory: {results_dir}\")\n",
    "    \n",
    "    # List available files\n",
    "    available_files = list(results_dir.glob(\"*\"))\n",
    "    print(f\"ÔøΩÔøΩ Available files:\")\n",
    "    for file in available_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    # Load comparison metrics\n",
    "    metrics_file = results_dir / \"comparison_metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        print(f\"\\nüìä Loaded comparison metrics:\")\n",
    "        print(f\"  Simple LSTM - Magnitude Accuracy: {metrics['simple_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "        print(f\"  Simple LSTM - Frequency Accuracy: {metrics['simple_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "        print(f\"  Attention LSTM - Magnitude Accuracy: {metrics['attention_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "        print(f\"  Attention LSTM - Frequency Accuracy: {metrics['attention_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "    \n",
    "    # Check if trained models exist\n",
    "    simple_model_path = results_dir / \"simple_lstm_model.pth\"\n",
    "    attention_model_path = results_dir / \"attention_lstm_model.pth\"\n",
    "    \n",
    "    if simple_model_path.exists() and attention_model_path.exists():\n",
    "        print(f\"\\n‚úÖ Found trained models:\")\n",
    "        print(f\"  Simple LSTM: {simple_model_path}\")\n",
    "        print(f\"  Attention LSTM: {attention_model_path}\")\n",
    "        return {\n",
    "            'models_dir': str(results_dir),\n",
    "            'simple_model_path': str(simple_model_path),\n",
    "            'attention_model_path': str(attention_model_path),\n",
    "            'metrics': metrics if 'metrics' in locals() else None\n",
    "        }\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Trained models not found\")\n",
    "        return None\n",
    "\n",
    "# Load results if available\n",
    "if output_dir:\n",
    "    results = load_comparison_results(output_dir)\n",
    "else:\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Overall Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    print(\"ÔøΩÔøΩ OVERALL COMPARISON METRICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Simple LSTM metrics\n",
    "    print(\"\\nüîµ Simple LSTM Performance:\")\n",
    "    print(f\"  Total Loss: {metrics['simple_lstm_metrics']['total_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Loss: {metrics['simple_lstm_metrics']['magnitude_loss']:.4f}\")\n",
    "    print(f\"  Frequency Loss: {metrics['simple_lstm_metrics']['frequency_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Accuracy: {metrics['simple_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "    print(f\"  Frequency Accuracy: {metrics['simple_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "    print(f\"  Magnitude Correlation: {metrics['simple_lstm_metrics']['magnitude_corr']:.3f}\")\n",
    "    print(f\"  Frequency Correlation: {metrics['simple_lstm_metrics']['frequency_corr']:.3f}\")\n",
    "    \n",
    "    # Attention LSTM metrics\n",
    "    print(\"\\nüü° Attention LSTM Performance:\")\n",
    "    print(f\"  Total Loss: {metrics['attention_lstm_metrics']['total_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Loss: {metrics['attention_lstm_metrics']['magnitude_loss']:.4f}\")\n",
    "    print(f\"  Frequency Loss: {metrics['attention_lstm_metrics']['frequency_loss']:.4f}\")\n",
    "    print(f\"  Magnitude Accuracy: {metrics['attention_lstm_metrics']['magnitude_accuracy']:.3f}\")\n",
    "    print(f\"  Frequency Accuracy: {metrics['attention_lstm_metrics']['frequency_accuracy']:.3f}\")\n",
    "    print(f\"  Magnitude Correlation: {metrics['attention_lstm_metrics']['magnitude_corr']:.3f}\")\n",
    "    print(f\"  Frequency Correlation: {metrics['attention_lstm_metrics']['frequency_corr']:.3f}\")\n",
    "    \n",
    "    # Hyperparameters used\n",
    "    print(\"\\n‚öôÔ∏è  Hyperparameters Used:\")\n",
    "    for key, value in metrics['hyperparameters'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"\\nÔøΩÔøΩ PERFORMANCE COMPARISON:\")\n",
    "    \n",
    "    # Magnitude accuracy comparison\n",
    "    simple_mag_acc = metrics['simple_lstm_metrics']['magnitude_accuracy']\n",
    "    attention_mag_acc = metrics['attention_lstm_metrics']['magnitude_accuracy']\n",
    "    \n",
    "    if simple_mag_acc > attention_mag_acc:\n",
    "        print(f\"  üéØ Simple LSTM wins on Magnitude Accuracy: {simple_mag_acc:.3f} vs {attention_mag_acc:.3f}\")\n",
    "    else:\n",
    "        print(f\"  üéØ Attention LSTM wins on Magnitude Accuracy: {attention_mag_acc:.3f} vs {simple_mag_acc:.3f}\")\n",
    "    \n",
    "    # Frequency accuracy comparison\n",
    "    simple_freq_acc = metrics['simple_lstm_metrics']['frequency_accuracy']\n",
    "    attention_freq_acc = metrics['attention_lstm_metrics']['frequency_accuracy']\n",
    "    \n",
    "    if simple_freq_acc > attention_freq_acc:\n",
    "        print(f\"  üéØ Simple LSTM wins on Frequency Accuracy: {simple_freq_acc:.3f} vs {attention_freq_acc:.3f}\")\n",
    "    else:\n",
    "        print(f\"  üéØ Attention LSTM wins on Frequency Accuracy: {attention_freq_acc:.3f} vs {simple_freq_acc:.3f}\")\n",
    "    \n",
    "    # Overall winner\n",
    "    simple_total = simple_mag_acc + simple_freq_acc\n",
    "    attention_total = attention_mag_acc + attention_freq_acc\n",
    "    \n",
    "    if simple_total > attention_total:\n",
    "        print(f\"\\nüèÜ OVERALL WINNER: Simple LSTM ({simple_total:.3f} vs {attention_total:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\nüèÜ OVERALL WINNER: Attention LSTM ({attention_total:.3f} vs {simple_total:.3f})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metrics available to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    # Create comparison plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    models = ['Simple LSTM', 'Attention LSTM']\n",
    "    mag_acc = [metrics['simple_lstm_metrics']['magnitude_accuracy'], \n",
    "               metrics['attention_lstm_metrics']['magnitude_accuracy']]\n",
    "    freq_acc = [metrics['simple_lstm_metrics']['frequency_accuracy'], \n",
    "                metrics['attention_lstm_metrics']['frequency_accuracy']]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, mag_acc, width, label='Magnitude Accuracy', alpha=0.8, color='skyblue')\n",
    "    ax1.bar(x + width/2, freq_acc, width, label='Frequency Accuracy', alpha=0.8, color='lightcoral')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Accuracy Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Loss Comparison\n",
    "    total_loss = [metrics['simple_lstm_metrics']['total_loss'], \n",
    "                  metrics['attention_lstm_metrics']['total_loss']]\n",
    "    mag_loss = [metrics['simple_lstm_metrics']['magnitude_loss'], \n",
    "                metrics['attention_lstm_metrics']['magnitude_loss']]\n",
    "    freq_loss = [metrics['simple_lstm_metrics']['frequency_loss'], \n",
    "                 metrics['attention_lstm_metrics']['frequency_loss']]\n",
    "    \n",
    "    ax2.bar(x - width/2, mag_loss, width, label='Magnitude Loss', alpha=0.8, color='lightblue')\n",
    "    ax2.bar(x + width/2, freq_loss, width, label='Frequency Loss', alpha=0.8, color='salmon')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Loss Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correlation Comparison\n",
    "    mag_corr = [metrics['simple_lstm_metrics']['magnitude_corr'], \n",
    "                metrics['attention_lstm_metrics']['magnitude_corr']]\n",
    "    freq_corr = [metrics['simple_lstm_metrics']['frequency_corr'], \n",
    "                 metrics['attention_lstm_metrics']['frequency_corr']]\n",
    "    \n",
    "    ax3.bar(x - width/2, mag_corr, width, label='Magnitude Correlation', alpha=0.8, color='lightgreen')\n",
    "    ax3.bar(x + width/2, freq_corr, width, label='Frequency Correlation', alpha=0.8, color='orange')\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Correlation')\n",
    "    ax3.set_title('Correlation Comparison')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(models)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Overall Performance Heatmap\n",
    "    performance_data = [\n",
    "        [mag_acc[0], freq_acc[0]],  # Simple LSTM\n",
    "        [mag_acc[1], freq_acc[1]]   # Attention LSTM\n",
    "    ]\n",
    "    \n",
    "    im = ax4.imshow(performance_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    ax4.set_xticks([0, 1])\n",
    "    ax4.set_xticklabels(['Magnitude', 'Frequency'])\n",
    "    ax4.set_yticks([0, 1])\n",
    "    ax4.set_yticklabels(['Simple LSTM', 'Attention LSTM'])\n",
    "    ax4.set_title('Performance Heatmap')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = ax4.text(j, i, f'{performance_data[i][j]:.3f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Comparison visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Metric': ['Total Loss', 'Magnitude Loss', 'Frequency Loss', 'Magnitude Accuracy', \n",
    "                   'Frequency Accuracy', 'Magnitude Correlation', 'Frequency Correlation'],\n",
    "        'Simple_LSTM': [\n",
    "            metrics['simple_lstm_metrics']['total_loss'],\n",
    "            metrics['simple_lstm_metrics']['magnitude_loss'],\n",
    "            metrics['simple_lstm_metrics']['frequency_loss'],\n",
    "            metrics['simple_lstm_metrics']['magnitude_accuracy'],\n",
    "            metrics['simple_lstm_metrics']['frequency_accuracy'],\n",
    "            metrics['simple_lstm_metrics']['magnitude_corr'],\n",
    "            metrics['simple_lstm_metrics']['frequency_corr']\n",
    "        ],\n",
    "        'Attention_LSTM': [\n",
    "            metrics['attention_lstm_metrics']['total_loss'],\n",
    "            metrics['attention_lstm_metrics']['magnitude_loss'],\n",
    "            metrics['attention_lstm_metrics']['frequency_loss'],\n",
    "            metrics['attention_lstm_metrics']['magnitude_accuracy'],\n",
    "            metrics['attention_lstm_metrics']['frequency_accuracy'],\n",
    "            metrics['attention_lstm_metrics']['magnitude_corr'],\n",
    "            metrics['attention_lstm_metrics']['frequency_corr']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = \"anti_overfitting_comparison_results.csv\"\n",
    "    summary_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "    print(\"\\nÔøΩÔøΩ Results Summary:\")\n",
    "    print(summary_df.round(4))\n",
    "    \n",
    "    # Calculate differences\n",
    "    summary_df['Difference'] = summary_df['Attention_LSTM'] - summary_df['Simple_LSTM']\n",
    "    summary_df['Simple_LSTM_Wins'] = summary_df['Difference'] < 0\n",
    "    \n",
    "    print(\"\\nüìä Performance Differences (Attention - Simple):\")\n",
    "    print(summary_df[['Metric', 'Difference']].round(4))\n",
    "    \n",
    "    # Count wins\n",
    "    simple_wins = summary_df['Simple_LSTM_Wins'].sum()\n",
    "    attention_wins = len(summary_df) - simple_wins\n",
    "    \n",
    "    print(f\"\\nüèÜ Final Score:\")\n",
    "    print(f\"  Simple LSTM wins: {simple_wins} metrics\")\n",
    "    print(f\"  Attention LSTM wins: {attention_wins} metrics\")\n",
    "    \n",
    "    if simple_wins > attention_wins:\n",
    "        print(f\"  üéØ OVERALL WINNER: Simple LSTM\")\n",
    "    elif attention_wins > simple_wins:\n",
    "        print(f\"  üéØ OVERALL WINNER: Attention LSTM\")\n",
    "    else:\n",
    "        print(f\"  üéØ TIE: Both models perform equally well\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metrics available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and results.get('metrics'):\n",
    "    metrics = results['metrics']\n",
    "    \n",
    "    print(\"üìã COMPREHENSIVE SUMMARY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Configuration: Anti-Overfitting Config\")\n",
    "    print(f\"Analysis Type: Overall Model Comparison\")\n",
    "    print()\n",
    "    \n",
    "    # Overall performance\n",
    "    print(\"üèÜ OVERALL PERFORMANCE:\")\n",
    "    simple_mag_acc = metrics['simple_lstm_metrics']['magnitude_accuracy']\n",
    "    simple_freq_acc = metrics['simple_lstm_metrics']['frequency_accuracy']\n",
    "    attention_mag_acc = metrics['attention_lstm_metrics']['magnitude_accuracy']\n",
    "    attention_freq_acc = metrics['attention_lstm_metrics']['frequency_accuracy']\n",
    "    \n",
    "    print(f\"  Simple LSTM:     {simple_mag_acc:.3f} magnitude, {simple_freq_acc:.3f} frequency\")\n",
    "    print(f\"  Attention LSTM:  {attention_mag_acc:.3f} magnitude, {attention_freq_acc:.3f} frequency\")\n",
    "    print()\n",
    "    \n",
    "    # Winner determination\n",
    "    simple_total = simple_mag_acc + simple_freq_acc\n",
    "    attention_total = attention_mag_acc + attention_freq_acc\n",
    "    \n",
    "    if attention_total > simple_total:\n",
    "        print(\"üéØ WINNER: Attention LSTM\")\n",
    "        print(f\"   Reason: Higher combined accuracy ({attention_total:.3f} vs {simple_total:.3f})\")\n",
    "    else:\n",
    "        print(\"üéØ WINNER: Simple LSTM\")\n",
    "        print(f\"   Reason: Higher combined accuracy ({simple_total:.3f} vs {attention_total:.3f})\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Anti-overfitting effectiveness\n",
    "    print(\"ÔøΩÔøΩÔ∏è  ANTI-OVERFITTING EFFECTIVENESS:\")\n",
    "    max_acc = max(simple_mag_acc, simple_freq_acc, attention_mag_acc, attention_freq_acc)\n",
    "    if max_acc < 0.9:\n",
    "        print(f\"  ‚úÖ Effective: Maximum accuracy is {max_acc:.3f} (below 0.9 threshold)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Caution: Maximum accuracy is {max_acc:.3f} (above 0.9 threshold)\")\n",
    "    \n",
    "    # Check for realistic performance ranges\n",
    "    simple_range = max(simple_mag_acc, simple_freq_acc) - min(simple_mag_acc, simple_freq_acc)\n",
    "    attention_range = max(attention_mag_acc, attention_freq_acc) - min(attention_mag_acc, attention_freq_acc)\n",
    "    \n",
    "    print(f\"  Simple LSTM accuracy range: {simple_range:.3f}\")\n",
    "    print(f\"  Attention LSTM accuracy range: {attention_range:.3f}\")\n",
    "    \n",
    "    if simple_range < 0.3 and attention_range < 0.3:\n",
    "        print(\"  ‚úÖ Good: Both models show consistent performance across tasks\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Caution: High variance across tasks may indicate instability\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"ÔøΩÔøΩ RECOMMENDATIONS:\")\n",
    "    if attention_total > simple_total + 0.1:\n",
    "        print(\"  ‚Ä¢ Attention LSTM shows significant improvement - consider for production\")\n",
    "    elif simple_total > attention_total + 0.1:\n",
    "        print(\"  ‚Ä¢ Simple LSTM performs better - simpler model may be sufficient\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Both models perform similarly - choose based on computational requirements\")\n",
    "    \n",
    "    if max_acc < 0.8:\n",
    "        print(\"  ‚Ä¢ Anti-overfitting config is working well - realistic performance achieved\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Consider further regularization if accuracy is too high\")\n",
    "    \n",
    "    print()\n",
    "    print(\"ÔøΩÔøΩ Analysis completed successfully!\")\n",
    "    print(\"\\nNote: This analysis shows overall model performance.\")\n",
    "    print(\"For per-bin accuracy analysis, you would need to run the trained models\")\n",
    "    print(\"on test data with spatial binning to get detailed per-bin metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "ipykernel": {
    "display_name": "Python 3",
    "language": "python"
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}