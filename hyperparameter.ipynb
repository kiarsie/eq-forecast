{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Current Best Performance:\n",
    "- Simple LSTM: Magnitude ±0.2 = 87.5%, Frequency range = 12.7%\n",
    "- Attention LSTM: Magnitude ±0.2 = 50.0%, Frequency range = 38.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install scikit-learn optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path for model imports\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "# Import your models\n",
    "try:\n",
    "    from models.shared_lstm_model import SharedLSTMModel, WeightedEarthquakeLoss\n",
    "    from models.attention_shared_lstm_model import AttentionSharedLSTMModel\n",
    "    from models.shared_lstm_trainer import SharedLSTMTrainer\n",
    "    from models.attention_shared_lstm_trainer import AttentionSharedLSTMTrainer\n",
    "    print(\"✅ Models imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the project root directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Hyperparameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Best Configuration (Baseline)\n",
    "BASELINE_CONFIG = {\n",
    "    # Model Architecture\n",
    "    'input_seq_features': 12,\n",
    "    'metadata_features': 4,\n",
    "    'lookback_years': 10,\n",
    "    'lstm_hidden_1': 64,\n",
    "    'lstm_hidden_2': 32,\n",
    "    'dense_hidden': 32,\n",
    "    'dropout_rate': 0.25,\n",
    "    'freq_head_type': 'linear',\n",
    "    \n",
    "    # Training Parameters\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_epochs': 300,\n",
    "    'patience': 15,\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    # Loss Weights\n",
    "    'magnitude_weight': 2.0,\n",
    "    'frequency_weight': 1.0,\n",
    "    'correlation_weight': 0.0,\n",
    "    \n",
    "    # Frequency Scaling\n",
    "    'frequency_scale_init': 10.0,\n",
    "    'frequency_bias_init': 2.0,\n",
    "    'scaling_lr_multiplier': 20,\n",
    "    'scaling_wd_multiplier': 3,\n",
    "    \n",
    "    # Training Stability\n",
    "    'gradient_clip': 0.5,\n",
    "    'scheduler_T0': 15,\n",
    "    'scheduler_T_mult': 2\n",
    "}\n",
    "\n",
    "print(\"📋 Baseline Configuration:\")\n",
    "for key, value in BASELINE_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search Spaces\n",
    "TUNING_SPACES = {\n",
    "    # Priority 1: Fix Frequency Range Compression\n",
    "    'frequency_scaling': {\n",
    "        'frequency_scale_init': [5.0, 10.0, 15.0, 20.0, 25.0],\n",
    "        'frequency_bias_init': [1.0, 2.0, 3.0, 5.0, 7.0],\n",
    "        'scaling_lr_multiplier': [15, 20, 25, 30, 35],\n",
    "        'scaling_wd_multiplier': [2, 3, 4, 5, 6]\n",
    "    },\n",
    "    \n",
    "    # Priority 2: Balance Loss Weights\n",
    "    'loss_weights': {\n",
    "        'magnitude_weight': [1.0, 1.5, 2.0, 2.5, 3.0],\n",
    "        'frequency_weight': [0.8, 1.0, 1.2, 1.5, 2.0]\n",
    "    },\n",
    "    \n",
    "    # Priority 3: Learning Rate Tuning\n",
    "    'learning_rates': {\n",
    "        'learning_rate': [3e-4, 5e-4, 7e-4, 1e-3, 1.5e-3],\n",
    "        'weight_decay': [5e-5, 1e-4, 2e-4, 5e-4]\n",
    "    },\n",
    "    \n",
    "    # Priority 4: Architecture Tuning\n",
    "    'architecture': {\n",
    "        'lstm_hidden_1': [48, 64, 80, 96],\n",
    "        'lstm_hidden_2': [24, 32, 40, 48],\n",
    "        'dense_hidden': [24, 32, 40, 48],\n",
    "        'dropout_rate': [0.15, 0.25, 0.35, 0.45]\n",
    "    },\n",
    "    \n",
    "    # Priority 5: Training Stability\n",
    "    'training_stability': {\n",
    "        'patience': [10, 15, 20, 25, 30],\n",
    "        'gradient_clip': [0.3, 0.5, 0.7, 1.0],\n",
    "        'scheduler_T0': [10, 15, 20, 25]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🔍 Tuning Search Spaces:\")\n",
    "for category, params in TUNING_SPACES.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for param, values in params.items():\n",
    "        print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Manual Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTester:\n",
    "    \"\"\"Class to test different hyperparameter combinations\"\"\"\n",
    "    \n",
    "    def __init__(self, base_config: Dict):\n",
    "        self.base_config = base_config.copy()\n",
    "        self.results = []\n",
    "        \n",
    "    def create_model(self, config: Dict, model_type: str = 'simple') -> nn.Module:\n",
    "        \"\"\"Create model with given configuration\"\"\"\n",
    "        if model_type == 'simple':\n",
    "            model = SharedLSTMModel(\n",
    "                input_seq_features=config['input_seq_features'],\n",
    "                metadata_features=config['metadata_features'],\n",
    "                lookback_years=config['lookback_years'],\n",
    "                lstm_hidden_1=config['lstm_hidden_1'],\n",
    "                lstm_hidden_2=config['lstm_hidden_2'],\n",
    "                dense_hidden=config['dense_hidden'],\n",
    "                dropout_rate=config['dropout_rate'],\n",
    "                freq_head_type=config['freq_head_type']\n",
    "            )\n",
    "        else:  # attention\n",
    "            model = AttentionSharedLSTMModel(\n",
    "                input_seq_features=config['input_seq_features'],\n",
    "                metadata_features=config['metadata_features'],\n",
    "                lookback_years=config['lookback_years'],\n",
    "                lstm_hidden_1=config['lstm_hidden_1'],\n",
    "                lstm_hidden_2=config['lstm_hidden_2'],\n",
    "                dense_hidden=config['dense_hidden'],\n",
    "                dropout_rate=config['dropout_rate'],\n",
    "                freq_head_type=config['freq_head_type']\n",
    "            )\n",
    "        \n",
    "        # Set custom frequency scaling parameters\n",
    "        if hasattr(model, 'frequency_scale'):\n",
    "            model.frequency_scale.data.fill_(config['frequency_scale_init'])\n",
    "        if hasattr(model, 'frequency_bias'):\n",
    "            model.frequency_bias.data.fill_(config['frequency_bias_init'])\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def test_configuration(self, config: Dict, model_type: str = 'simple') -> Dict:\n",
    "        \"\"\"Test a single configuration\"\"\"\n",
    "        print(f\"\\n🧪 Testing {model_type.upper()} LSTM with config:\")\n",
    "        for key, value in config.items():\n",
    "            if key in self.base_config and value != self.base_config[key]:\n",
    "                print(f\"  {key}: {self.base_config[key]} → {value}\")\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            model = self.create_model(config, model_type)\n",
    "            \n",
    "            # Create dummy data for testing\n",
    "            batch_size = 4\n",
    "            input_sequence = torch.randn(batch_size, config['lookback_years'], config['input_seq_features'])\n",
    "            metadata = torch.randn(batch_size, config['metadata_features'])\n",
    "            \n",
    "            # Test forward pass\n",
    "            with torch.no_grad():\n",
    "                magnitude_pred, frequency_pred = model(input_sequence, metadata)\n",
    "            \n",
    "            # Calculate prediction ranges\n",
    "            mag_range = magnitude_pred.max().item() - magnitude_pred.min().item()\n",
    "            freq_range = frequency_pred.max().item() - frequency_pred.min().item()\n",
    "            \n",
    "            # Count parameters\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            result = {\n",
    "                'config': config.copy(),\n",
    "                'model_type': model_type,\n",
    "                'magnitude_range': mag_range,\n",
    "                'frequency_range': freq_range,\n",
    "                'total_params': total_params,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ Test successful!\")\n",
    "            print(f\"  Magnitude range: {mag_range:.4f}\")\n",
    "            print(f\"  Frequency range: {freq_range:.4f}\")\n",
    "            print(f\"  Total parameters: {total_params:,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'config': config.copy(),\n",
    "                'model_type': model_type,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            print(f\"❌ Test failed: {e}\")\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def run_grid_search(self, param_grid: Dict, model_type: str = 'simple') -> List[Dict]:\n",
    "        \"\"\"Run grid search over parameter combinations\"\"\"\n",
    "        print(f\"\\n🚀 Starting grid search for {model_type.upper()} LSTM...\")\n",
    "        \n",
    "        # Generate all combinations\n",
    "        import itertools\n",
    "        keys = list(param_grid.keys())\n",
    "        values = list(param_grid.values())\n",
    "        combinations = list(itertools.product(*values))\n",
    "        \n",
    "        print(f\"Total combinations to test: {len(combinations)}\")\n",
    "        \n",
    "        for i, combination in enumerate(combinations):\n",
    "            config = self.base_config.copy()\n",
    "            for key, value in zip(keys, combination):\n",
    "                config[key] = value\n",
    "            \n",
    "            print(f\"\\n--- Test {i+1}/{len(combinations)} ---\")\n",
    "            self.test_configuration(config, model_type)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_best_configs(self, metric: str = 'frequency_range', top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Get top-k configurations based on a metric\"\"\"\n",
    "        successful_results = [r for r in self.results if r['status'] == 'success']\n",
    "        \n",
    "        if not successful_results:\n",
    "            return []\n",
    "        \n",
    "        # Sort by metric (higher is better for ranges)\n",
    "        sorted_results = sorted(successful_results, key=lambda x: x[metric], reverse=True)\n",
    "        \n",
    "        return sorted_results[:top_k]\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot results summary\"\"\"\n",
    "        successful_results = [r for r in self.results if r['status'] == 'success']\n",
    "        \n",
    "        if not successful_results:\n",
    "            print(\"No successful results to plot\")\n",
    "            return\n",
    "        \n",
    "        # Create DataFrame for plotting\n",
    "        df = pd.DataFrame(successful_results)\n",
    "        \n",
    "        # Plot frequency range vs magnitude range\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.scatter(df['frequency_range'], df['magnitude_range'], alpha=0.7)\n",
    "        plt.xlabel('Frequency Range')\n",
    "        plt.ylabel('Magnitude Range')\n",
    "        plt.title('Frequency vs Magnitude Range')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.hist(df['frequency_range'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Frequency Range')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Frequency Range Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.hist(df['magnitude_range'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Magnitude Range')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Magnitude Range Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.scatter(df['total_params'], df['frequency_range'], alpha=0.7)\n",
    "        plt.xlabel('Total Parameters')\n",
    "        plt.ylabel('Frequency Range')\n",
    "        plt.title('Parameters vs Frequency Range')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show top configurations\n",
    "        print(\"\\n🏆 Top 5 Configurations by Frequency Range:\")\n",
    "        top_configs = self.get_best_configs('frequency_range', 5)\n",
    "        for i, config in enumerate(top_configs):\n",
    "            print(f\"\\n{i+1}. Frequency Range: {config['frequency_range']:.4f}\")\n",
    "            print(f\"   Magnitude Range: {config['magnitude_range']:.4f}\")\n",
    "            print(f\"   Model Type: {config['model_type']}\")\n",
    "            print(f\"   Key Changes:\")\n",
    "            for key, value in config['config'].items():\n",
    "                if key in self.base_config and value != self.base_config[key]:\n",
    "                    print(f\"     {key}: {self.base_config[key]} → {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Test 1: Frequency Scaling Parameters (Priority 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tester\n",
    "tester = HyperparameterTester(BASELINE_CONFIG)\n",
    "\n",
    "# Test frequency scaling parameters\n",
    "print(\"🎯 Testing Frequency Scaling Parameters (Priority 1)\")\n",
    "print(\"Goal: Increase frequency prediction range from 12.7% to 40-60%\")\n",
    "\n",
    "# Test simple LSTM first\n",
    "frequency_scaling_results = tester.run_grid_search(\n",
    "    TUNING_SPACES['frequency_scaling'], \n",
    "    model_type='simple'\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\n📊 Frequency Scaling Results:\")\n",
    "tester.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Test 2: Loss Weight Balancing (Priority 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss weight combinations\n",
    "print(\"🎯 Testing Loss Weight Balancing (Priority 2)\")\n",
    "print(\"Goal: Balance magnitude vs frequency performance\")\n",
    "\n",
    "# Use best frequency scaling config from previous test\n",
    "best_freq_config = tester.get_best_configs('frequency_range', 1)[0]\n",
    "print(f\"\\nUsing best frequency scaling config:\")\n",
    "print(f\"Frequency range: {best_freq_config['frequency_range']:.4f}\")\n",
    "\n",
    "# Update base config with best frequency scaling\n",
    "updated_config = BASELINE_CONFIG.copy()\n",
    "for key, value in best_freq_config['config'].items():\n",
    "    if key in TUNING_SPACES['frequency_scaling']:\n",
    "        updated_config[key] = value\n",
    "\n",
    "# Create new tester with updated config\n",
    "tester2 = HyperparameterTester(updated_config)\n",
    "\n",
    "# Test loss weights\n",
    "loss_weight_results = tester2.run_grid_search(\n",
    "    TUNING_SPACES['loss_weights'], \n",
    "    model_type='simple'\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\n📊 Loss Weight Results:\")\n",
    "tester2.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Test 3: Learning Rate Optimization (Priority 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test learning rates\n",
    "print(\"🎯 Testing Learning Rate Optimization (Priority 3)\")\n",
    "print(\"Goal: Find optimal learning rate for training stability\")\n",
    "\n",
    "# Use best config from previous tests\n",
    "best_overall_config = tester2.get_best_configs('frequency_range', 1)[0]\n",
    "print(f\"\\nUsing best overall config:\")\n",
    "print(f\"Frequency range: {best_overall_config['frequency_range']:.4f}\")\n",
    "print(f\"Magnitude range: {best_overall_config['magnitude_range']:.4f}\")\n",
    "\n",
    "# Update config with best parameters\n",
    "final_config = updated_config.copy()\n",
    "for key, value in best_overall_config['config'].items():\n",
    "    if key in TUNING_SPACES['loss_weights']:\n",
    "        final_config[key] = value\n",
    "\n",
    "# Create final tester\n",
    "tester3 = HyperparameterTester(final_config)\n",
    "\n",
    "# Test learning rates\n",
    "lr_results = tester3.run_grid_search(\n",
    "    TUNING_SPACES['learning_rates'], \n",
    "    model_type='simple'\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\n📊 Learning Rate Results:\")\n",
    "tester3.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Test 4: Attention Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention model with best parameters\n",
    "print(\"🎯 Testing Attention Model with Best Parameters\")\n",
    "print(\"Goal: See if attention model can match simple LSTM performance\")\n",
    "\n",
    "# Get best config from all tests\n",
    "best_config = tester3.get_best_configs('frequency_range', 1)[0]\n",
    "print(f\"\\nBest config from simple LSTM:\")\n",
    "print(f\"Frequency range: {best_config['frequency_range']:.4f}\")\n",
    "print(f\"Magnitude range: {best_config['magnitude_range']:.4f}\")\n",
    "\n",
    "# Test attention model with same config\n",
    "attention_tester = HyperparameterTester(best_config['config'])\n",
    "attention_result = attention_tester.test_configuration(\n",
    "    best_config['config'], \n",
    "    model_type='attention'\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n📊 Simple LSTM vs Attention LSTM Comparison:\")\n",
    "print(f\"Simple LSTM:\")\n",
    "print(f\"  Frequency range: {best_config['frequency_range']:.4f}\")\n",
    "print(f\"  Magnitude range: {best_config['magnitude_range']:.4f}\")\n",
    "print(f\"  Total parameters: {best_config['total_params']:,}\")\n",
    "\n",
    "print(f\"\\nAttention LSTM:\")\n",
    "print(f\"  Frequency range: {attention_result['frequency_range']:.4f}\")\n",
    "print(f\"  Magnitude range: {attention_result['magnitude_range']:.4f}\")\n",
    "print(f\"  Total parameters: {attention_result['total_params']:,}\")\n",
    "\n",
    "# Calculate improvements\n",
    "freq_improvement = ((attention_result['frequency_range'] - best_config['frequency_range']) / best_config['frequency_range']) * 100\n",
    "mag_improvement = ((attention_result['magnitude_range'] - best_config['magnitude_range']) / best_config['magnitude_range']) * 100\n",
    "\n",
    "print(f\"\\n📈 Improvements with Attention:\")\n",
    "print(f\"  Frequency range: {freq_improvement:+.1f}%\")\n",
    "print(f\"  Magnitude range: {mag_improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations():\n",
    "    \"\"\"Generate final hyperparameter recommendations\"\"\"\n",
    "    print(\"🎯 FINAL HYPERPARAMETER RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get best configs from all testers\n",
    "    all_results = []\n",
    "    if hasattr(tester, 'results'):\n",
    "        all_results.extend(tester.results)\n",
    "    if hasattr(tester2, 'results'):\n",
    "        all_results.extend(tester2.results)\n",
    "    if hasattr(tester3, 'results'):\n",
    "        all_results.extend(tester3.results)\n",
    "    \n",
    "    successful_results = [r for r in all_results if r['status'] == 'success']\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"❌ No successful results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Find best configurations\n",
    "    best_freq = max(successful_results, key=lambda x: x['frequency_range'])\n",
    "    best_mag = max(successful_results, key=lambda x: x['magnitude_range'])\n",
    "    best_balanced = max(successful_results, key=lambda x: x['frequency_range'] + x['magnitude_range'])\n",
    "    \n",
    "    print(\"\\n🏆 BEST FREQUENCY PREDICTION:\")\n",
    "    print(f\"  Frequency range: {best_freq['frequency_range']:.4f}\")\n",
    "    print(f\"  Magnitude range: {best_freq['magnitude_range']:.4f}\")\n",
    "    print(f\"  Model type: {best_freq['model_type']}\")\n",
    "    print(\"  Key parameters:\")\n",
    "    for key, value in best_freq['config'].items():\n",
    "        if key in BASELINE_CONFIG and value != BASELINE_CONFIG[key]:\n",
    "            print(f\"    {key}: {BASELINE_CONFIG[key]} → {value}\")\n",
    "    \n",
    "    print(\"\\n🏆 BEST MAGNITUDE PREDICTION:\")\n",
    "    print(f\"  Frequency range: {best_mag['frequency_range']:.4f}\")\n",
    "    print(f\"  Magnitude range: {best_mag['magnitude_range']:.4f}\")\n",
    "    print(f\"  Model type: {best_mag['model_type']}\")\n",
    "    print(\"  Key parameters:\")\n",
    "    for key, value in best_mag['config'].items():\n",
    "        if key in BASELINE_CONFIG and value != BASELINE_CONFIG[key]:\n",
    "            print(f\"    {key}: {BASELINE_CONFIG[key]} → {value}\")\n",
    "    \n",
    "    print(\"\\n🏆 BEST BALANCED PERFORMANCE:\")\n",
    "    print(f\"  Frequency range: {best_balanced['frequency_range']:.4f}\")\n",
    "    print(f\"  Magnitude range: {best_balanced['magnitude_range']:.4f}\")\n",
    "    print(f\"  Model type: {best_balanced['model_type']}\")\n",
    "    print(\"  Key parameters:\")\n",
    "    for key, value in best_balanced['config'].items():\n",
    "        if key in BASELINE_CONFIG and value != BASELINE_CONFIG[key]:\n",
    "            print(f\"    {key}: {BASELINE_CONFIG[key]} → {value}\")\n",
    "    \n",
    "    # Generate config files\n",
    "    print(\"\\n💾 GENERATING CONFIGURATION FILES:\")\n",
    "    \n",
    "    configs = {\n",
    "        'best_frequency.json': best_freq['config'],\n",
    "        'best_magnitude.json': best_mag['config'],\n",
    "        'best_balanced.json': best_balanced['config']\n",
    "    }\n",
    "    \n",
    "    for filename, config in configs.items():\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"  ✅ {filename} saved\")\n",
    "    \n",
    "    print(\"\\n🚀 NEXT STEPS:\")\n",
    "    print(\"  1. Use the best configurations in your main training script\")\n",
    "    print(\"  2. Retrain models with optimized parameters\")\n",
    "    print(\"  3. Compare final performance on test set\")\n",
    "    print(\"  4. Consider ensemble approach for production\")\n",
    "\n",
    "# Generate recommendations\n",
    "generate_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Advanced Tuning with Optuna (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced hyperparameter optimization with Optuna\n",
    "try:\n",
    "    import optuna\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "        \n",
    "        # Suggest hyperparameters\n",
    "        config = {\n",
    "            'frequency_scale_init': trial.suggest_float('freq_scale', 5.0, 30.0),\n",
    "            'frequency_bias_init': trial.suggest_float('freq_bias', 1.0, 10.0),\n",
    "            'magnitude_weight': trial.suggest_float('mag_weight', 1.0, 4.0),\n",
    "            'frequency_weight': trial.suggest_float('freq_weight', 0.5, 3.0),\n",
    "            'learning_rate': trial.suggest_float('lr', 1e-4, 2e-3, log=True),\n",
    "            'dropout_rate': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'lstm_hidden_1': trial.suggest_categorical('lstm1', [48, 64, 80, 96]),\n",
    "            'lstm_hidden_2': trial.suggest_categorical('lstm2', [24, 32, 40, 48])\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Test configuration\n",
    "            test_tester = HyperparameterTester(BASELINE_CONFIG)\n",
    "            result = test_tester.test_configuration(config, 'simple')\n",
    "            \n",
    "            if result['status'] == 'success':\n",
    "                # Optimize for frequency range (primary) and magnitude range (secondary)\n",
    "                score = result['frequency_range'] * 0.7 + result['magnitude_range'] * 0.3\n",
    "                return score\n",
    "            else:\n",
    "                return -1000  # Penalty for failed configurations\n",
    "                \n",
    "        except Exception as e:\n",
    "            return -1000  # Penalty for errors\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    print(\"🚀 Starting Optuna optimization...\")\n",
    "    print(\"This will take some time but will find optimal parameters automatically!\")\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(objective, n_trials=50, timeout=300)  # 50 trials or 5 minutes\n",
    "    \n",
    "    print(\"\\n🏆 Optuna Optimization Results:\")\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best value: {study.best_trial.value:.4f}\")\n",
    "    print(f\"Best params: {study.best_trial.params}\")\n",
    "    \n",
    "    # Plot optimization history\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "    optuna.visualization.plot_param_importances(study)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"❌ Optuna not installed. Install with: pip install optuna\")\n",
    "    print(\"Manual tuning results above are still very effective!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## �� Summary\n",
    "\n",
    "This notebook provides comprehensive hyperparameter tuning for your earthquake forecasting model:\n",
    "\n",
    "### ✅ **What We've Implemented:**\n",
    "1. **Manual Grid Search** - Test specific parameter combinations\n",
    "2. **Priority-Based Tuning** - Focus on most impactful parameters first\n",
    "3. **Performance Tracking** - Monitor frequency and magnitude ranges\n",
    "4. **Configuration Management** - Save and load best configurations\n",
    "5. **Advanced Optimization** - Optional Optuna integration\n",
    "\n",
    "### �� **Key Tuning Areas:**\n",
    "1. **Frequency Scaling** - Fix range compression (Priority 1)\n",
    "2. **Loss Weights** - Balance magnitude vs frequency (Priority 2)\n",
    "3. **Learning Rates** - Training stability (Priority 3)\n",
    "4. **Architecture** - LSTM sizes, MLP funnel (Priority 4)\n",
    "5. **Training Stability** - Patience, regularization (Priority 5)\n",
    "\n",
    "### 🚀 **Next Steps:**\n",
    "1. Run the tuning tests above\n",
    "2. Use the best configurations in your main training script\n",
    "3. Retrain models with optimized parameters\n",
    "4. Compare final performance\n",
    "5. Consider ensemble approach\n",
    "\n",
    "Happy tuning! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
